{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESOURCES\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/use_cases/question_answering/quickstart/\n",
    "\n",
    "https://python.langchain.com/docs/tutorials/rag/\n",
    "\n",
    "https://scalexi.medium.com/implementing-a-retrieval-augmented-generation-rag-system-with-openais-api-using-langchain-ab39b60b4d9f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and save [Langchain API key](https://docs.smith.langchain.com/how_to_guides/setup/create_account_api_key) to `.env`. \n",
    "\n",
    "[LangSmith](https://www.langchain.com/langsmith) is a helpful monitoring tool developed by LangChain to trace the behavior of your application. This is particularly helpful when your application has RAG and history. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup the API keys\n",
    "load_dotenv()\n",
    "\n",
    "## If you want LangSmith to trace your runs, set this environmental variable\n",
    "#os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide context-rich data to the interior designer, I created a text repository comprising of fake expert data.\n",
    "I used ChatGPT to create the fake data.\n",
    "For easy handling, separate text files were created for different areas of the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for reading text files containing information for RAG\n",
    "def read_txt_files_in_folder(folder_path):\n",
    "    all_texts = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    filtered_content = ''.join([char for char in content if char not in ['**','#','##','###']])\n",
    "                    all_texts.append(filtered_content)\n",
    "    \n",
    "    return all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the text files\n",
    "text = read_txt_files_in_folder('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(text))\n",
    "# print(len(text[0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text data should be split into manageable chunks that fit within the context window of the model.\n",
    "`RecursiveCharacterTextSplitter` recursively splits the text data into fragments using characters from the default list `[\"\\n\\n\", \"\\n\", \" \", \"\"]`, by finding the one that works. Chunks are created so that they are less than or equal in length to `chunk_size`.\n",
    "While there are several other text splitters, [this splitter](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/) works best for generic text. \n",
    "\n",
    "`create_documents` is a little confusing since we have already processed the text documents into an array of strings. This method takes the array of string data as input and returns a set of 'document' objects that contain the split chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200, \n",
    "    add_start_index=True\n",
    ")\n",
    "\n",
    "## Converting text data into documents\n",
    "docs = text_splitter.create_documents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(docs))\n",
    "# print(docs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have domain specific information, loaded from text files and processed into a format suitable for LangChain. For each user query, we should retrieve the appropriate snippets and provide them as context to the model. The RAG process is only as good as the retrieved snippets' relevance and quality. LangChain has [implementations](https://python.langchain.com/v0.2/docs/concepts/#retrieval) of multiple retrieval techniques that are suitable for different usecases.  \n",
    "\n",
    "Here, I use vector stores, one of the the simplest methods of retrieval. This is a beginner friendly method. Specifically, [Chroma](https://python.langchain.com/docs/integrations/vectorstores/chroma/) vector database was used to prepare the vector store. Here, unstructured text data is transformed into embeddings and during query phase, the query is converted to an embedding, the appropriate snippets are retrieved based on embedding similarity and an index corresponding to the relevant chunk is returned. Embeddings are computed using [OpenAI embedding models](https://python.langchain.com/docs/integrations/text_embedding/openai/).\n",
    "\n",
    "It is important to note that, in addition to the retrieval method, the size of the chunks and overlap used during text splitting play a key role on the effectiveness of RAG inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=docs, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of an LLM will be only as good as the prompt we give. [LangChain Hub](https://smith.langchain.com/hub) consists of pre-defined prompts for diverse usecases. created a prompt template based on `rag-prompt` from the Hub.   \n",
    "[PromptTemplate](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/quick_start/) converts the string prompt to a LangChain prompt template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Template based on hub.pull(\"rlm/rag-prompt\")\n",
    "template = \"\"\"Use the following pieces of context to answer the questions related to interior and exterior design of homes. Please respond without using double-quotation marks. \n",
    "If the question is not related to interior or exterior design, politely say that your are an assistant helping with interior and exterior design and tell the user to ask relavant questions, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(custom_rag_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the LLM, I use OpenAI's `gpt-4o` model through `ChatOpenAI` API of LangChain. To try other OpenAI models, you can simply update the `model_name` argument with a different model. Check [OpenAI Plaform](https://platform.openai.com/docs/models) for all available models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.7, model_name=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all the components required query our AI interior designer model. Now we create a chain that composes all the components and functions together. We use `RunnablePassthrough` to pass the user query into the prompt.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What color scheme should I use in a model kitchen?\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Out of context question\n",
    "rag_chain.invoke(\"What is the capital of the United States?\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding history to the chat\n",
    "Here, I also experiment with LangChain's buil-in chain constructors to create chains.\n",
    "[Reference](https://python.langchain.com/docs/tutorials/qa_chat_history/) \n",
    "\n",
    "https://python.langchain.com/v0.1/docs/modules/chains/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.messages import AIMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with buil-in chain constructors and without history\n",
    "\n",
    "Note that here I am using ChatPromptTemplate for creating the prompt. This is suitable for prompting chat models (that is, we have back-and-forth). We can input a list of chat messages and assign roles to the messages. [Learn more about prompt templates](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/quick_start/)\n",
    "\n",
    "`create_stuff_documents_chain` create a chain for passing a list of Documents to a model. [link](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using ChatPromptTemplate\n",
    "system_prompt = (\"Use the following pieces of context to answer the questions related to interior and exterior design of homes.\" \n",
    "                 \"Please respond without using double-quotation marks.\" \n",
    "                 \"If the question is not related to interior or exterior design, politely say that you don't know and tell the user to ask relavant questions,\" \n",
    "                 \"don't try to make up an answer.\"\n",
    "                 \"Use three sentences maximum and keep the answer as concise as possible.\"\n",
    "                 \"\\n\\n\"\n",
    "                 \"{context}\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "## This chain passes a list of documents to the LLM\n",
    "qna_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(retriever, qna_chain)\n",
    "\n",
    "response = rag_chain.invoke({\"input\": \"What color scheme should I use in a bathroom?\"})\n",
    "print(response[\"answer\"])\n",
    "\n",
    "response = rag_chain.invoke({\"input\": \"What color dress should I wear to a party?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding chat history\n",
    "\n",
    "This part of the notebook closely follows this [tutorial](https://python.langchain.com/docs/tutorials/qa_chat_history/) from LangChain.\n",
    " \n",
    "When incorporating history, the retriever should retrieve documents based on current context and history. So, use another LLM call to create a prompt that incorporates current query and chat history to create the retriever.\n",
    "\n",
    "We must rephrase the input query to incorporate historical messages. We use a sub-chain to create a ['history aware'](https://python.langchain.com/docs/tutorials/qa_chat_history/#adding-chat-history) retriever which we then use with the LLM to get the response.\n",
    "\n",
    "[create_history_aware_retriever](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.history_aware_retriever.create_history_aware_retriever.html) creates a chain that takes input and conversation history and returns documents.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a prompt by combining chat history and user query\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the `question_answer_chain` again and build the final rag chain to apply the `history_aware_retriever` and `question_answer_chain` in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "qna_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, qna_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before invoking the chain, we must manage `\"chat_history\"`. This is maintained as a list.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "question = \"What kind of design theme should I use for my living room?\"\n",
    "ai_msg1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(ai_msg1[\"answer\"])\n",
    "print()\n",
    "\n",
    "question2 = \"What kind of furniture should I use there?\"\n",
    "\n",
    "## Checking the intermediate output of the rephrasing prompt step\n",
    "out_chain = (\n",
    "    contextualize_q_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "temp_out = out_chain.invoke({\"input\": question2, \"chat_history\": chat_history})\n",
    "\n",
    "print(temp_out)\n",
    "print()\n",
    "\n",
    "ai_msg2 = rag_chain.invoke({\"input\": question2, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg2[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We developed the logic for retaining chat history between messages. However, this is still very manual. We can wrap our chat model in a [LangGraph](https://langchain-ai.github.io/langgraph/) application to automatically persist the message history.   \n",
    "\n",
    "[Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) layers are crucial when maintaining memory within chat applications. LangGraph has is own built-in persistence layer.  \n",
    "\n",
    "We should install LangGraph separately.\n",
    "\n",
    "`pip install -U langgraph`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dict `State` represents the state of the application. It has the same input and output keys as `rag_chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    input: str\n",
    "    chat_history: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    context: str\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the function (or node) that calls the model, in other words, runs the `rag_chain`.\n",
    "This function updates the graph state by updating the chat history with the input message and response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: State):\n",
    "    response = rag_chain.invoke(state)\n",
    "\n",
    "    ## Invoking rag_chain outputs a model response and updates the state. So the function 'returns' \n",
    "    ## the new graph state.\n",
    "    return {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(state[\"input\"]),\n",
    "            AIMessage(response[\"answer\"]),\n",
    "        ],\n",
    "        \"context\":response[\"context\"],\n",
    "        \"answer\": response[\"answer\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the graph and compile it with a checkpointer object.\n",
    "We can choose where we would like to persist the state. In this case, we choose the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=State)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "## Compiling\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A chat application should handle interactions with multiple users.\n",
    "\n",
    "This application supports multiple conversations through multiple threads, where each thread has a unique indentifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "result = app.invoke(\n",
    "    {\"input\": \"What material should I use for kitchen cabinets?\"},\n",
    "    config = config,\n",
    ")\n",
    "\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = app.invoke(\n",
    "    {\"input\": \"What are reclaimed woods?\"},\n",
    "    config = config,\n",
    ")\n",
    "\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = app.invoke(\n",
    "    {\"input\": \"Why are they trendy?\"},\n",
    "    config = config,\n",
    ")\n",
    "\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what happens when I use a different thread ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config2 = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "\n",
    "## Eventhough this has a different thread ID, sometimes we get an answer because RAG appends relevant context if there are relevant words in the questions.\n",
    "result = app.invoke(\n",
    "    {\"input\": \"Why are they trendy?\"},\n",
    "    config = config2,\n",
    ")\n",
    "\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the chat history using `get_state` as shown below. \n",
    "\n",
    "The tutorial has an [illustration](https://python.langchain.com/docs/tutorials/qa_chat_history/#tying-it-together) that ties everything together and gives you the big picture of the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = app.get_state(config).values[\"chat_history\"]\n",
    "for message in chat_history:\n",
    "    message.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
