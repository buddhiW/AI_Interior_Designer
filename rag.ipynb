{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESOURCES\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/use_cases/question_answering/quickstart/\n",
    "\n",
    "https://python.langchain.com/docs/tutorials/rag/\n",
    "\n",
    "https://scalexi.medium.com/implementing-a-retrieval-augmented-generation-rag-system-with-openais-api-using-langchain-ab39b60b4d9f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and save [Langchain API key](https://docs.smith.langchain.com/how_to_guides/setup/create_account_api_key) to `.env`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup the API keys\n",
    "load_dotenv()\n",
    "\n",
    "## If you want LangSmith to trace your runs, set this flag to true\n",
    "#os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide context-rich data to the interior designer, I created a text repository comprising of fake expert data.\n",
    "I used ChatGPT to create the fake data.\n",
    "For easy handling, separate text files were created for different areas of the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for reading text files containing information for RAG\n",
    "def read_txt_files_in_folder(folder_path):\n",
    "    all_texts = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    filtered_content = ''.join([char for char in content if char not in ['**','#','##','###']])\n",
    "                    all_texts.append(filtered_content)\n",
    "    \n",
    "    return all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the text files\n",
    "text = read_txt_files_in_folder('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(text))\n",
    "# print(len(text[0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text data should be split into manageable chunks that fit within the context window of the model.\n",
    "`RecursiveCharacterTextSplitter` recursively splits the text data into fragments using characters from the default list `[\"\\n\\n\", \"\\n\", \" \", \"\"]`, by finding the one that works. Chunks are created so that they are less than or equal in length to `chunk_size`.\n",
    "While there are several other text splitters, [this splitter](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/) works best for generic text. \n",
    "\n",
    "`create_documents` is a little confusing since we have already processed the text documents into an array of strings. This method takes the array of string data as input and returns a set of 'document' objects that contain the split chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200, \n",
    "    add_start_index=True\n",
    ")\n",
    "\n",
    "## Converting text data into documents\n",
    "docs = text_splitter.create_documents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(docs))\n",
    "# print(docs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have domain specific information, loaded from text files and processed into a format suitable for LangChain. For each user query, we should retrieve the appropriate snippets and provide them as context to the model. The RAG process is only as good as the retrieved snippets' relevance and quality. LangChain has [implementations](https://python.langchain.com/v0.2/docs/concepts/#retrieval) of multiple retrieval techniques that are suitable for different usecases.  \n",
    "\n",
    "Here, I use vector stores, one of the the simplest methods of retrieval. This is a beginner friendly method. Specifically, [Chroma](https://python.langchain.com/docs/integrations/vectorstores/chroma/) vector database was used to prepare the vector store. Here, unstructured text data is transformed into embeddings and during query phase, the query is converted to an embedding, the appropriate snippets are retrieved based on embedding similarity and an index corresponding to the relevant chunk is returned. Embeddings are computed using [OpenAI embedding models](https://python.langchain.com/docs/integrations/text_embedding/openai/).\n",
    "\n",
    "It is important to note that, in addition to the retrieval method, the size of the chunks and overlap used during text splitting play a key role on the effectiveness of RAG inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=docs, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of an LLM will be only as good as the prompt we give. [LangChain Hub](https://smith.langchain.com/hub) consists of pre-defined prompts for diverse usecases. created a prompt template based on `rag-prompt` from the Hub.   \n",
    "[PromptTemplate](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/quick_start/) converts the string prompt to a LangChain prompt template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Template based on hub.pull(\"rlm/rag-prompt\")\n",
    "template = \"\"\"Use the following pieces of context to answer the questions related to interior and exterior design of homes. Please respond without using double-quotation marks. \n",
    "If the question is not related to interior or exterior design, politely say that your are an assistant helping with interior and exterior design and tell the user to ask relavant questions, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(custom_rag_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the LLM, I use OpenAI's `gpt-4o` model through `ChatOpenAI` API of LangChain. To try other OpenAI models, you can simply update the `model_name` argument with a different model. Check [OpenAI Plaform](https://platform.openai.com/docs/models) for all available models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.7, model_name=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all the components required query our AI interior designer model. Now we create a chain that composes all the components and functions together. We use `RunnablePassthrough` to pass the user query into the prompt.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Consider using bold color schemes with deep hues such as rich navy blues, forest greens, and charcoal grays to make a statement in your model kitchen. Balance these bold colors with lighter countertops or backsplashes to create depth and drama. Adding brass or gold fixtures can further enhance the luxurious feel.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What color scheme should I use in a model kitchen?\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm an assistant specializing in interior and exterior design. Please ask questions related to those topics for assistance.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Out of context question\n",
    "rag_chain.invoke(\"What is the capital of the United States?\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding history to the chat\n",
    "Here, I also experiment with LangChain's buil-in chain constructors to create chains.\n",
    "[Reference](https://python.langchain.com/docs/tutorials/qa_chat_history/) \n",
    "\n",
    "https://python.langchain.com/v0.1/docs/modules/chains/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.messages import AIMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with buil-in chain constructors and without history\n",
    "\n",
    "Note that here I am using ChatPromptTemplate for creating the prompt. This is suitable for prompting chat models (that is, we have back-and-forth). We can input a list of chat messages and assign roles to the messages. [Learn more about prompt templates](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/quick_start/)\n",
    "\n",
    "`create_stuff_documents_chain` create a chain for passing a list of Documents to a model. [link](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know, but you may consider using serene earth tones and soft neutrals like warm beiges, soft taupes, and gentle greens. These colors create a calming environment, perfect for a bathroom where you can relax and unwind.\n",
      "I don't know. Please ask me questions related to interior or exterior design of homes.\n"
     ]
    }
   ],
   "source": [
    "## Using ChatPromptTemplate\n",
    "system_prompt = (\"Use the following pieces of context to answer the questions related to interior and exterior design of homes.\" \n",
    "                 \"Please respond without using double-quotation marks.\" \n",
    "                 \"If the question is not related to interior or exterior design, politely say that you don't know and tell the user to ask relavant questions,\" \n",
    "                 \"don't try to make up an answer.\"\n",
    "                 \"Use three sentences maximum and keep the answer as concise as possible.\"\n",
    "                 \"\\n\\n\"\n",
    "                 \"{context}\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "## This chain passes a list of documents to the LLM\n",
    "qna_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(retriever, qna_chain)\n",
    "\n",
    "response = rag_chain.invoke({\"input\": \"What color scheme should I use in a bathroom?\"})\n",
    "print(response[\"answer\"])\n",
    "\n",
    "response = rag_chain.invoke({\"input\": \"What color dress should I wear to a party?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding chat history\n",
    "\n",
    "This part of the notebook closely follows this [tutorial](https://python.langchain.com/docs/tutorials/qa_chat_history/) from LangChain.\n",
    " \n",
    "When incorporating history, the retriever should retrieve documents based on current context and history. So, use another LLM call to create a prompt that incorporates current query and chat history to create the retriever.\n",
    "\n",
    "We must rephrase the input query to incorporate historical messages. We use a sub-chain to create a ['history aware'](https://python.langchain.com/docs/tutorials/qa_chat_history/#adding-chat-history) retriever which we then use with the LLM to get the response.\n",
    "\n",
    "[create_history_aware_retriever](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.history_aware_retriever.create_history_aware_retriever.html) creates a chain that takes input and conversation history and returns documents.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a prompt by combining chat history and user query\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the `question_answer_chain` again and build the final rag chain to apply the `history_aware_retriever` and `question_answer_chain` in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "qna_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, qna_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before invoking the chain, we must manage `\"chat_history\"`. This is maintained as a list.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider using a design theme that incorporates earthy color palettes with natural tones, like terracotta and muted greens, for a warm and inviting atmosphere. You could also explore biophilic design by adding indoor greenery and natural materials to create a serene environment. Mixing modern and vintage styles is another trend for 2024, allowing you to blend sleek, contemporary elements with charming vintage pieces.\n",
      "\n",
      "What kind of furniture should I use in my living room?\n",
      "\n",
      "Opt for curved furniture and soft shapes to create an inviting atmosphere, such as rounded sofas and oval coffee tables. Consider multifunctional pieces like modular sofas or expandable tables to maximize your space and versatility. Additionally, look for furniture made from recycled or reclaimed materials for a sustainable and unique touch.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "question = \"What kind of design theme should I use for my living room?\"\n",
    "ai_msg1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(ai_msg1[\"answer\"])\n",
    "print()\n",
    "\n",
    "question2 = \"What kind of furniture should I use there?\"\n",
    "\n",
    "## Checking the intermediate output of the rephrasing prompt step\n",
    "out_chain = (\n",
    "    contextualize_q_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "temp_out = out_chain.invoke({\"input\": question2, \"chat_history\": chat_history})\n",
    "\n",
    "print(temp_out)\n",
    "print()\n",
    "\n",
    "ai_msg2 = rag_chain.invoke({\"input\": question2, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg2[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We developed the logic for retaining chat history between messages. However, this is still very manual. We can wrap our chat model in a [LangGraph](https://langchain-ai.github.io/langgraph/) application to automatically persist the message history.   \n",
    "\n",
    "[Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) layers are crucial when maintaining memory within chat applications. LangGraph has is own built-in persistence layer.  \n",
    "\n",
    "We should install LangGraph separately.\n",
    "\n",
    "`pip install -U langgraph`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dict `State` represents the state of the application. It has the same input and output keys as `rag_chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    input: str\n",
    "    chat_history: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    context: str\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the function (or node) that calls the model, in other words, runs the `rag_chain`.\n",
    "This function updates the graph state by updating the chat history with the input message and response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: State):\n",
    "    response = rag_chain.invoke(state)\n",
    "\n",
    "    ## Invoking rag_chain outputs a model response and updates the state. So the function 'returns' \n",
    "    ## the new graph state.\n",
    "    return {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(state[\"input\"]),\n",
    "            AIMessage(response[\"answer\"]),\n",
    "        ],\n",
    "        \"context\":response[\"context\"],\n",
    "        \"answer\": response[\"answer\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the graph and compile it with a checkpointer object.\n",
    "We can choose where we would like to persist the state. In this case, we choose the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=State)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "## Compiling\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A chat application should handle interactions with multiple users.\n",
    "\n",
    "This application supports multiple conversations through multiple threads, where each thread has a unique indentifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kitchen cabinets, consider using sustainable materials like reclaimed wood or bamboo for an eco-friendly choice. Mixing materials, such as pairing natural wood with metal accents, can create a contemporary yet warm vibe. These options align with 2024's trends and offer durability and style.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "result = app.invoke(\n",
    "    {\"input\": \"What material should I use for kitchen cabinets?\"},\n",
    "    config = config,\n",
    ")\n",
    "\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reclaimed woods are repurposed woods that have been salvaged from old structures like barns, factories, or warehouses. They are valued for their unique character, sustainability, and environmental benefits, often featuring a weathered appearance that adds charm to interior spaces.\n"
     ]
    }
   ],
   "source": [
    "result = app.invoke(\n",
    "    {\"input\": \"What are reclaimed woods?\"},\n",
    "    config = config,\n",
    ")\n",
    "\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reclaimed woods are trendy because they offer a sustainable option by repurposing existing materials, reducing the need for new resources. Their unique character and history add charm and authenticity to interior spaces. Additionally, they align with the growing focus on eco-friendly and sustainable design practices.\n"
     ]
    }
   ],
   "source": [
    "result = app.invoke(\n",
    "    {\"input\": \"Why are they trendy?\"},\n",
    "    config = config,\n",
    ")\n",
    "\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what happens when I use a different thread ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know which specific trend you're referring to. Please ask about a particular trend related to interior or exterior design so I can provide a relevant answer.\n"
     ]
    }
   ],
   "source": [
    "config2 = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "\n",
    "## Eventhough this has a different thread ID, sometimes we get an answer because RAG appends relevant context if there are relevant words in the questions.\n",
    "result = app.invoke(\n",
    "    {\"input\": \"Why are they trendy?\"},\n",
    "    config = config2,\n",
    ")\n",
    "\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the chat history using `get_state` as shown below. \n",
    "\n",
    "The tutorial has an [illustration](https://python.langchain.com/docs/tutorials/qa_chat_history/#tying-it-together) that ties everything together and gives you the big picture of the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What material should I use for kitchen cabinets?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "For kitchen cabinets, consider using sustainable materials like reclaimed wood or bamboo for an eco-friendly choice. Mixing materials, such as pairing natural wood with metal accents, can create a contemporary yet warm vibe. These options align with 2024's trends and offer durability and style.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What are reclaimed woods?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Reclaimed woods are repurposed woods that have been salvaged from old structures like barns, factories, or warehouses. They are valued for their unique character, sustainability, and environmental benefits, often featuring a weathered appearance that adds charm to interior spaces.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Why are they trendy?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Reclaimed woods are trendy because they offer a sustainable option by repurposing existing materials, reducing the need for new resources. Their unique character and history add charm and authenticity to interior spaces. Additionally, they align with the growing focus on eco-friendly and sustainable design practices.\n"
     ]
    }
   ],
   "source": [
    "chat_history = app.get_state(config).values[\"chat_history\"]\n",
    "for message in chat_history:\n",
    "    message.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
